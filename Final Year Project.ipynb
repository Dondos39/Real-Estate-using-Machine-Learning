{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(7)\n",
    "import kerastuner as kt\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "#data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import skew\n",
    "\n",
    "#feature selection\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#models\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner.tuners import Hyperband, BayesianOptimization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Dondo\\Desktop\\NYC.csv', index_col=0)\n",
    "\n",
    "\n",
    "#Normalize labels\n",
    "labelEncoder = sklearn.preprocessing.LabelEncoder()\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of duplicates across the dataframe\n",
    "sum(df.duplicated(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates\n",
    "df = df.drop_duplicates(df.columns,keep = 'last')\n",
    "sum(df.duplicated(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix \n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[np.number]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping column as it is empty\n",
    "df = df.drop(['EASE-MENT'], 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartment Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping column as it is almost empty\n",
    "df = df.drop(['APARTMENT NUMBER'], 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SALE PRICE'] = pd.to_numeric(df['SALE PRICE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SALE PRICE'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between SALE PRICE and the rest of the columns\n",
    "corr = df.corr()\n",
    "corr['SALE PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# Plot the data and configure the settings\n",
    "sns.boxplot(x='SALE PRICE', data=df)\n",
    "plt.ticklabel_format(style='sci', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values that are >100,000 and <5.000.000\n",
    "df = df[(df['SALE PRICE'] > 100000) & (df['SALE PRICE'] < 5000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Size\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# Plot the data and configure the settings\n",
    "sns.boxplot(x='SALE PRICE', data=df)\n",
    "plt.ticklabel_format(style='sci', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the size of the plot\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Plot the data of skewness\n",
    "sns.distplot(df['SALE PRICE'])\n",
    "plt.title('Histogram of SALE PRICE in USD')\n",
    "plt.ylabel('Normed Frequency')\n",
    "plt.show()\n",
    "df['SALE PRICE'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Square Feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the Land Square Feet below to number value via pandas function\n",
    "df['LAND SQUARE FEET'] = pd.to_numeric(df['LAND SQUARE FEET'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "df['LAND SQUARE FEET'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing values with mean of the column\n",
    "df['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].fillna(df['LAND SQUARE FEET'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "df['LAND SQUARE FEET'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='LAND SQUARE FEET', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})\n",
    "sns.set(font_scale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['LAND SQUARE FEET'] != 0) & (df['LAND SQUARE FEET'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='LAND SQUARE FEET', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross Square Feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the Land Square Feet below to number value via pandas function\n",
    "df['GROSS SQUARE FEET']= pd.to_numeric(df['GROSS SQUARE FEET'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "df['GROSS SQUARE FEET'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing values with mean of the column\n",
    "df['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].fillna(df['GROSS SQUARE FEET'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "df['GROSS SQUARE FEET'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='GROSS SQUARE FEET', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep properties with fewer than 10.000 Gross Square Feet\n",
    "df = df[(df['GROSS SQUARE FEET'] != 0) & (df['GROSS SQUARE FEET'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='GROSS SQUARE FEET', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='TOTAL UNITS',y='SALE PRICE', data=df)\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.title('Total Units vs Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "df['TOTAL UNITS'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exlude property with 2261 Total Units value and 0\n",
    "df = df[(df['TOTAL UNITS'] > 0) & (df['TOTAL UNITS'] != 2261)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove data where commercial + residential doesn't equal total units\n",
    "df = df[df['TOTAL UNITS'] == df['COMMERCIAL UNITS'] + df['RESIDENTIAL UNITS']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='TOTAL UNITS',y='SALE PRICE', data=df)\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.title('Total Units vs Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residential Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='RESIDENTIAL UNITS', y='SALE PRICE', data=df)\n",
    "plt.title('Residential Units vs Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commerical Units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='COMMERCIAL UNITS', y='SALE PRICE', data=df)\n",
    "plt.title('Commercial Units vs Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Built "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check null values\n",
    "df['YEAR BUILT'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='YEAR BUILT', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Building that were built in year 0\n",
    "df = df[df['YEAR BUILT'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='YEAR BUILT', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='ZIP CODE', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Zip Code that are 0\n",
    "df = df[df['ZIP CODE'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.regplot(x='ZIP CODE', y='SALE PRICE', data=df, fit_reg=False, scatter_kws={'alpha':0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(exclude=[np.number]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borough "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.subplots(figsize=(10,6))\n",
    "\n",
    "sns.boxplot(x='BOROUGH', y='SALE PRICE', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='NEIGHBORHOOD', values='SALE PRICE', aggfunc=np.median)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because it has a lot of uniques\n",
    "df = df.drop(['NEIGHBORHOOD'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tax Class At Present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='TAX CLASS AT PRESENT', values='SALE PRICE', aggfunc=np.median)\n",
    "pivot.plot(kind ='bar', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tax Class At Time of Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='TAX CLASS AT TIME OF SALE', values='SALE PRICE', aggfunc=np.median)\n",
    "pivot.plot(kind ='bar', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Class Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='BUILDING CLASS CATEGORY', values='SALE PRICE', aggfunc=np.median)\n",
    "pivot.plot(kind ='bar', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Class At Present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='BUILDING CLASS AT PRESENT', values='SALE PRICE', aggfunc=np.median)\n",
    "pivot.plot(kind ='bar', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Class at Time of Sale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='BUILDING CLASS AT TIME OF SALE', values='SALE PRICE', aggfunc=np.median)\n",
    "pivot.plot(kind ='bar', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot=df.pivot_table(index='ADDRESS', values='SALE PRICE', aggfunc=np.median)\n",
    "print(\"Percentage of Uniques: \" + str(int(100 * (df['ADDRESS'].nunique() / len(df['ADDRESS'])))) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['ADDRESS'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sale Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert SALE DATE to datetime\n",
    "df['SALE DATE'] = df['SALE DATE'].str.rstrip('00:00:00')\n",
    "df['SALE DATE'] = pd.to_datetime(df['SALE DATE'])  \n",
    "x = df['SALE DATE'].dt.quarter\n",
    "\n",
    "#Plot\n",
    "plt.subplots(figsize=(10,6))\n",
    "sns.boxplot(x, y = 'SALE PRICE', data=df)\n",
    "plt.title('Sale Price Distribution by Quarter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop because all quarters have very similar distribution compared to SALE PRICE \n",
    "df = df.drop(['SALE DATE'], 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only numberic values\n",
    "numeric_data=df.select_dtypes(include=[np.number])\n",
    "numeric_data.describe()\n",
    "numeric_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranforming skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = df[numeric_data.columns].apply(lambda x: skew(x.dropna().astype(float)))\n",
    "skewed = skewed[skewed > 1]\n",
    "skewed = skewed.index\n",
    "df[skewed] = np.log1p(df[skewed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(df[numeric_data.columns])\n",
    "\n",
    "scaled = scaler.transform(df[numeric_data.columns])\n",
    "\n",
    "for i, col in enumerate(numeric_data.columns):\n",
    "       df[col] = scaled[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BOROUGH'] = labelEncoder.fit_transform(df['BOROUGH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tax Class at Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT PRESENT'] = labelEncoder.fit_transform(df['TAX CLASS AT PRESENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tax Class at Time of Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TAX CLASS AT TIME OF SALE'] = labelEncoder.fit_transform(df['TAX CLASS AT TIME OF SALE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Class at Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS AT PRESENT'] = labelEncoder.fit_transform(df['BUILDING CLASS AT PRESENT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Class Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS CATEGORY'] = labelEncoder.fit_transform(df['BUILDING CLASS CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Class at Time of Sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BUILDING CLASS AT TIME OF SALE'] = labelEncoder.fit_transform(df['BUILDING CLASS AT TIME OF SALE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df.loc[:, ['SALE PRICE']]\n",
    "features = df.loc[:, ['BOROUGH', 'BUILDING CLASS CATEGORY', 'TAX CLASS AT PRESENT', 'BLOCK', 'LOT', 'BUILDING CLASS AT PRESENT',\n",
    "                      'ZIP CODE', 'RESIDENTIAL UNITS', 'COMMERCIAL UNITS', 'TOTAL UNITS', 'LAND SQUARE FEET', 'GROSS SQUARE FEET',\n",
    "                      'YEAR BUILT', 'TAX CLASS AT TIME OF SALE' , 'BUILDING CLASS AT TIME OF SALE']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFECV(LinearRegression(), step = 1, cv = 10, scoring = \"neg_mean_squared_error\")\n",
    "selector = selector.fit(X_train, y_train)\n",
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, selector.support_ == True]\n",
    "X_test = X_test.loc[:, selector.support_ == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(lr, X_test, y_test, cv = 10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supprt Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "svr = SVR()\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(svr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "grid = {'C': np.arange(0, 10, 0.05),\n",
    "              'epsilon':np.arange(0, 0.5, 0.05),\n",
    "              'gamma': ['auto'],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "tuned_svr = RandomizedSearchCV(svr,\n",
    "                                 grid,\n",
    "                                 n_iter = 14,\n",
    "                                 cv = 5,\n",
    "                                 scoring = \"neg_mean_squared_error\", \n",
    "                                 random_state = 42,\n",
    "                                 n_jobs = 1)\n",
    "\n",
    "# Fit the random search model\n",
    "tuned_svr.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "print(\"Time Elapsed: {} minutes\".format((t1 - t0)/60))\n",
    "tuned_svr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "svr = SVR(**tuned_svr.best_params_)\n",
    "\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(svr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))\n",
    "\n",
    "train_sizes ,train_scores, valid_scores = learning_curve(svr, \n",
    "                                                         X_train, \n",
    "                                                         y_train, \n",
    "                                                         train_sizes=[1, 100, 500, 1000, 10000, 20000],\n",
    "                                                         cv=10,\n",
    "                                                         scoring = 'neg_mean_squared_error')\n",
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "valid_scores_mean = -valid_scores.mean(axis = 1)\n",
    "pd.Series(train_scores_mean, index = train_sizes)\n",
    "pd.Series(valid_scores_mean, index = train_sizes)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, valid_scores_mean, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(dtr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "grid = {'max_depth': np.arange(0, 1000, 100),\n",
    "        'min_samples_split': np.arange(0, 1, 0.01),\n",
    "        'min_samples_leaf': np.arange(0, 2000, 10),\n",
    "        'max_features': np.arange(0, 1, 0.01),}\n",
    "\n",
    "tuned_dtr = RandomizedSearchCV(dtr,\n",
    "                                 grid,\n",
    "                                 n_iter = 20000,\n",
    "                                 cv = 5,\n",
    "                                 scoring=\"neg_mean_squared_error\", \n",
    "                                 n_jobs = 1)\n",
    "\n",
    "# Fit the random search model\n",
    "tuned_dtr.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "print(\"Time Elapsed: {} minutes\".format((t1 - t0)/60))\n",
    "tuned_dtr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "dtr = DecisionTreeRegressor(min_samples_split=0.001,\n",
    " min_samples_leaf=10,\n",
    " max_features=0.7000000000000001,\n",
    " max_depth=100)\n",
    "\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(dtr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))\n",
    "\n",
    "train_sizes ,train_scores, valid_scores = learning_curve(dtr, \n",
    "                                                         X_train, \n",
    "                                                         y_train, \n",
    "                                                         train_sizes=[1, 1000, 5000, 10000, 20000],\n",
    "                                                         cv=10,\n",
    "                                                         scoring = 'neg_mean_squared_error')\n",
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "valid_scores_mean = -valid_scores.mean(axis = 1)\n",
    "pd.Series(train_scores_mean, index = train_sizes)\n",
    "pd.Series(valid_scores_mean, index = train_sizes)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, valid_scores_mean, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "en = ElasticNet()\n",
    "\n",
    "en.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(en, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "grid = {'alpha': np.arange(0.001, 0.1, 0.001),\n",
    "        'l1_ratio': np.arange(0.01, 1, 0.01),\n",
    "        'max_iter': np.arange(100, 2000, 10),\n",
    "        'tol': np.arange(0.001, 0.1, 0.001) }\n",
    "\n",
    "tuned_en = RandomizedSearchCV(en,\n",
    "                                 grid,\n",
    "                                 n_iter = 30000,\n",
    "                                 cv = 5,\n",
    "                                 scoring=\"neg_mean_squared_error\",\n",
    "                                 n_jobs = 1)\n",
    "\n",
    "# Fit the random search model\n",
    "tuned_en.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "print(\"Time Elapsed: {} minutes\".format((t1 - t0)/60))\n",
    "tuned_en.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "en = ElasticNet(tol=0.031, \n",
    "                max_iter=710, \n",
    "                l1_ratio=0.4, \n",
    "                alpha=0.1)\n",
    "\n",
    "en.fit(X_train, y_train)\n",
    "\n",
    "y_pred = en.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(en, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))\n",
    "\n",
    "train_sizes ,train_scores, valid_scores = learning_curve(en, \n",
    "                                                         X_train, \n",
    "                                                         y_train, \n",
    "                                                         train_sizes=[1, 100, 500, 1000, 10000, 17521],\n",
    "                                                         cv=10,\n",
    "                                                         scoring = 'neg_mean_squared_error')\n",
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "valid_scores_mean = -valid_scores.mean(axis = 1)\n",
    "pd.Series(train_scores_mean, index = train_sizes)\n",
    "pd.Series(valid_scores_mean, index = train_sizes)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, valid_scores_mean, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reservel 1000 samples for validation\n",
    "X_val = X_train[-1000:]\n",
    "y_val = y_train[-1000:]\n",
    "X_train = X_train[:-1000]\n",
    "y_train = y_train[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "        \n",
    "        #Input\n",
    "        layers.Dense(8),\n",
    "        \n",
    "        #Output\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"SGD\", loss=\"mae\", metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_mse', patience = 10)\n",
    "t0 = time.time()\n",
    "model = get_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 4000, validation_data = (X_val, y_val), callbacks = [es], verbose = 0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = model.evaluate(X_test, y_test,batch_size = 1, verbose = 0)\n",
    "t1 = time.time()\n",
    "\n",
    "print(results[1])\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNHyperModel(HyperModel):\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        \n",
    "        #Input Layer\n",
    "        model.add(layers.Dense(self.input_shape, \n",
    "                               activation = hp.Choice('activation_1', values = ['relu', 'selu', 'elu'])))\n",
    "        \n",
    "        #Hidden Layer\n",
    "        model.add(layers.Dense(units = hp.Int('units_1', 0, 500, 5), \n",
    "                         activation = hp.Choice('activation_2', values = ['relu', 'selu', 'elu'])))\n",
    "        #Hidden Layer\n",
    "        model.add(layers.Dense(units = hp.Int('units_2', 0, 500, 5), \n",
    "                         activation = hp.Choice('activation_3', values = ['relu', 'selu', 'elu'])))\n",
    "        #Hidden Layer\n",
    "        model.add(layers.Dense(units = hp.Int('units_3', 0, 500, 5), \n",
    "                         activation = hp.Choice('activation_4', values = ['relu', 'selu', 'elu'])))\n",
    "        #Output\n",
    "        model.add(layers.Dense(1,\n",
    "                         activation = hp.Choice('activation_5', values = ['relu', 'selu', 'elu'])))\n",
    "    \n",
    "        model.compile(optimizer = hp.Choice('optimizer', values = ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']),\n",
    "                      loss = \"MAE\", \n",
    "                      metrics = ['MSE'])\n",
    "  \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = kt.HyperParameters()\n",
    "\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    NNHyperModel(len(X_train.columns)),\n",
    "    max_trials = 60,\n",
    "    executions_per_trial = 1,\n",
    "    objective='val_MSE',\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_MSE', patience = 2)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs = 400, validation_data = (X_test, y_test), verbose = 1, callbacks=[es])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Input Layer Activation: %s\\n\" % (best_hps.get('activation_1')))\n",
    "print(\"Hidden Layer 1: \\nUnits: %s \\nActivation: %s\\n\" % (best_hps.get('units_1'), best_hps.get('activation_2')))\n",
    "print(\"Hidden Layer 2: \\nUnits: %s \\nActivation: %s\\n\" % (best_hps.get('units_2'), best_hps.get('activation_3')))\n",
    "print(\"Hidden Layer 3: \\nUnits: %s \\nActivation: %s\\n\" % (best_hps.get('units_3'), best_hps.get('activation_4')))\n",
    "print(\"Output Layer Activation: %s\\n\" % (best_hps.get('activation_5')))\n",
    "print(\"Optimizer: %s\\n\" % (best_hps.get('optimizer')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "        \n",
    "        #Input Layer\n",
    "        layers.Dense(8, activation = 'selu'),\n",
    "        \n",
    "        #Hidden layer 1\n",
    "        layers.Dense(275, activation = 'relu'),\n",
    "        \n",
    "        #Hidden layer 2\n",
    "        layers.Dense(330, activation = 'elu'),\n",
    "        \n",
    "        #Hidden layer 3\n",
    "        layers.Dense(285, activation = 'relu'),\n",
    "        \n",
    "        #Output Layer\n",
    "        layers.Dense(1, activation = 'selu')\n",
    "    ])\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mae\", metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_mse', patience = 20)\n",
    "\n",
    "t0 = time.time()\n",
    "model = get_model()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 4000, validation_data = (X_val, y_val), callbacks = [es], verbose = 0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = model.evaluate(X_test, y_test, batch_size = 1, verbose = 0)\n",
    "t1 = time.time()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "print(\"10-fold score: {}\".format(results[1]))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history.history['mse'], label='train')\n",
    "plt.plot(history.history['val_mse'], label='test')\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(rfr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# Create the random grid\n",
    "grid = {'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': [10, 20, 40, 60, 80],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "tuned_rfr = RandomizedSearchCV(rfr,\n",
    "                                 grid,\n",
    "                                 n_iter = 60,\n",
    "                                 cv = 2,\n",
    "                                 scoring=\"neg_mean_squared_error\", \n",
    "                                 n_jobs = 1)\n",
    "\n",
    "# Fit the random search model\n",
    "tuned_rfr.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "print(\"Time Elapsed: {} minutes\".format((t1 - t0)/60))\n",
    "tuned_rfr.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "rfr = RandomForestRegressor(n_estimators=600, min_samples_split=5, min_samples_leaf=10, max_features='auto', max_depth=20)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(rfr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))\n",
    "\n",
    "train_sizes ,train_scores, valid_scores = learning_curve(rfr, \n",
    "                                                         X_train, \n",
    "                                                         y_train, \n",
    "                                                         train_sizes=[1, 100, 500, 1000, 10000, 20000],\n",
    "                                                         cv=10,\n",
    "                                                         scoring = 'neg_mean_squared_error')\n",
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "valid_scores_mean = -valid_scores.mean(axis = 1)\n",
    "pd.Series(train_scores_mean, index = train_sizes)\n",
    "pd.Series(valid_scores_mean, index = train_sizes)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, valid_scores_mean, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "xgbr = xgb.XGBRegressor()\n",
    "\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgbr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(xgbr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "grid = {'eta': [0.3, 0.2, 0.1, 0.05, 0.01, 0.005],\n",
    "        'gamma': np.arange(0, 1, 0.01),\n",
    "        'max_depth': np.arange(5, 15, 1),\n",
    "        'min_child_weight': np.arange(1, 10, 1),\n",
    "        'subsample': np.arange(0.5, 1.5, 0.1),\n",
    "        'colsample_bytree': np.arange(0.5, 1.5, 0.1)}\n",
    "\n",
    "tuned_xgb = RandomizedSearchCV(xgbr,\n",
    "                                 grid,\n",
    "                                 n_iter = 2000,\n",
    "                                 cv = 2,\n",
    "                                 scoring=\"neg_mean_squared_error\", \n",
    "                                 n_jobs = 1)\n",
    "\n",
    "# Fit the random search model\n",
    "tuned_xgb.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Time Elapsed: {} minutes\".format((t1 - t0)/ 60))\n",
    "tuned_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "xgbr = xgb.XGBRegressor(subsample = 0.9,\n",
    "                        min_child_weight=8,\n",
    "                        max_depth=5,\n",
    "                        gamma=0.78,\n",
    "                        eta=0.2,\n",
    "                        colsample_bytree=0.7)\n",
    "\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgbr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(xgbr, X_test, y_test, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))\n",
    "\n",
    "train_sizes ,train_scores, valid_scores = learning_curve(xgbr, \n",
    "                                                         X_train, \n",
    "                                                         y_train, \n",
    "                                                         train_sizes=[1, 100, 500, 1000, 10000, 17518],\n",
    "                                                         cv=10,\n",
    "                                                         scoring = 'neg_mean_squared_error')\n",
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "valid_scores_mean = -valid_scores.mean(axis = 1)\n",
    "pd.Series(train_scores_mean, index = train_sizes)\n",
    "pd.Series(valid_scores_mean, index = train_sizes)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, valid_scores_mean, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('svm', SVR(**tuned_svr.best_params_)))\n",
    "    \n",
    "    # define meta learner model\n",
    "    level1 = LinearRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "stacked_Regressor = get_stacking()\n",
    "\n",
    "stacked_Regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = stacked_Regressor.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(stacked_Regressor, X_test, y_test, cv = 10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = {'SVR': svr.predict(X_test),\n",
    "                     'DTR':dtr.predict(X_test),\n",
    "                     'ENet':en.predict(X_test),\n",
    "                     'NN': model.predict(X_test).reshape(-1),\n",
    "                     'RFR':rfr.predict(X_test), \n",
    "                     'XGB':xgbr.predict(X_test)}\n",
    "\n",
    "mp = pd.DataFrame(data=model_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (22, 8))\n",
    "sns.set(font_scale=3)\n",
    "sns.heatmap(mp.corr(), annot = True, cmap ='BrBG', ax = ax, fmt='.2f', linewidths = 0.05, annot_kws = {'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking():\n",
    "\n",
    "    level0 = list()\n",
    "    level0.append(('rfr', rfr))\n",
    "    level0.append(('xgb', xgbr))\n",
    "    \n",
    "   \n",
    "    level1 = LinearRegression()\n",
    "\n",
    "    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "stacked_Regressor = get_stacking()\n",
    "\n",
    "stacked_Regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = stacked_Regressor.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(stacked_Regressor, X_test, y_test, cv = 10, scoring=\"neg_mean_squared_error\")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "\n",
    "print(\"Average 10-Fold CV Score: {}\".format(np.mean(scores)))\n",
    "print(\"Time Elapsed: {} seconds\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_featute_importance(model, title):\n",
    "    sns.set(font_scale = 1)\n",
    "    importance = pd.DataFrame({'Features': X_train.columns, 'Importance': model.feature_importances_})\n",
    "    importance = importance.set_index('Features')\n",
    "    g = sns.scatterplot(x = importance.index, y = importance.Importance, marker = \"X\", label = title)\n",
    "    g.set_xticklabels(X_train.columns, rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_featute_importance(xgbr, 'XGBoost')\n",
    "plot_featute_importance(rfr, 'RForest')\n",
    "plot_featute_importance(dtr, 'DTR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance1(model, title):\n",
    "    model.fit(X_train, y_train)\n",
    "    importance = pd.DataFrame({'Features':X_train.columns, 'Importance':np.transpose(model.coef_)})\n",
    "    importance = importance.set_index('Features')\n",
    "    g = sns.scatterplot(x = importance.index, y = importance.Importance, marker = \"X\", label = title)\n",
    "    g.set_xticklabels(X_train.columns, rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance1(en, 'ENet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes ,train_scores, valid_scores = learning_curve(stacked_Regressor, \n",
    "                                                         X_train, \n",
    "                                                         y_train, \n",
    "                                                         train_sizes=[1, 100, 500, 1000, 10000, 19000],\n",
    "                                                         cv=10,\n",
    "                                                         scoring = 'neg_mean_squared_error')\n",
    "train_scores_mean = -train_scores.mean(axis = 1)\n",
    "valid_scores_mean = -valid_scores.mean(axis = 1)\n",
    "pd.Series(train_scores_mean, index = train_sizes)\n",
    "pd.Series(valid_scores_mean, index = train_sizes)\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, valid_scores_mean, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.61, 0.45, 0.60, 0.43, 0.99, 0.64, 0.65, 0.41, 0.37, 0.36, 0.36, 0.34, 0.45, 0.35]\n",
    "y = [136, 92, 0.57, 0.27, 0.07, 0.09, 17, 64, 32, 136, 5, 8, 400, 196]\n",
    "z = np.arange(0, 14, 1)\n",
    "labels = ['SVR','TunedSVR','DecisionTree', 'TunedDT', 'Elastic Net', 'TunedEN', 'NN',\n",
    "          'TunedNN', 'Random Forest', 'TunedRF', 'XGBoost', 'TunedXGB', 'Stacking', 'TunedS']\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "ax = plt.subplot(111)\n",
    "ax2=ax.twinx()\n",
    "\n",
    "ax.bar(z-0.2, x, width = 0.2, color = \"blue\", label = \"MSE\")\n",
    "ax2.bar(labels, y, width = 0.2, color = \"red\", label = \"Time\")\n",
    "\n",
    "ax.set_xlabel(\"Model\",fontsize = 29)\n",
    "ax.set_ylabel(\"Mean Squared Error\", color = \"blue\",fontsize = 38)\n",
    "ax2.set_ylabel(\"Time Elapsed\", color = \"red\", fontsize = 38)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.68, 0.66, 0.74, 0.64, 0.64, 0.8, 0.8, 0.63, 0.59, 0.58, 0.57, 0.57,0.67 ,0.58]\n",
    "y = [136, 92, 0.57, 0.27, 0.07, 0.09, 17, 64, 32, 136, 5, 8, 400, 196]\n",
    "z = np.arange(0, 14, 1)\n",
    "\n",
    "labels = ['SVR','TunedSVR','DecisionTree', 'TunedDT', 'Elastic Net', 'TunedEN', 'NN',\n",
    "          'TunedNN', 'Random Forest', 'TunedRF', 'XGBoost', 'TunedXGB', 'Stacked', 'TunedS']\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "ax = plt.subplot(111)\n",
    "ax2=ax.twinx()\n",
    "\n",
    "ax.bar(z-0.2, x, width = 0.2, color = \"blue\", label = \"MSE\")\n",
    "ax2.bar(labels, y, width = 0.2, color = \"red\", label = \"Time\")\n",
    "\n",
    "ax.set_xlabel(\"Model\",fontsize = 29)\n",
    "ax.set_ylabel(\"Root Mean Squared Error\", color = \"blue\",fontsize = 38)\n",
    "ax2.set_ylabel(\"Time Elapsed\", color = \"red\", fontsize = 38)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "realValue = np.exp(y_pred) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realValue.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(realValue.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
